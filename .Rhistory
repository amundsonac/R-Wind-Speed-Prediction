dat
dat
dat
dat <- read.csv(
file = "C:/Users/aaron/OneDrive/Documents/GitHub/R-Wind-Speed-Prediction/training_r.csv",
header = TRUE,
stringsAsFactors = FALSE
)
View(dat)
View(dat)
help(xgboost)
??xgboost
## libraries
require(readr)
require(xgboost)
require(Matrix)
require(data.table)
if (!require('vcd')) {
install.packages('vcd')
require(vcd)
}
library(caret)
## Read in training data attributes from CSV file
trainingDat <- read.csv(
file = "training_task2.csv",
header = TRUE,
stringsAsFactors = FALSE
)
## Read in testing data attributes from CSV file
testingDat <- read.csv(
file = "testing_r.csv",
header = TRUE,
stringsAsFactors = FALSE
)
## Transform into data table
training <- data.table(trainingDat, keep.rownames = F)
testing <- data.table(testingDat, keep.rownames = F)
testingDat <- read.csv(
file = "testing_task2.csv",
header = TRUE,
stringsAsFactors = FALSE
)
training <- data.table(trainingDat, keep.rownames = F)
testing <- data.table(testingDat, keep.rownames = F)
linReg <- xgboost(data = data.matrix(training),
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
linReg <- xgboost(data = trainingDat,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
linReg <- xgboost(data = training,
label = NULL,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
View(training)
training$label
linReg <- xgboost(data = training,
label = NULL,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
training <- as.matrix(trainingDat,as.numeric)
training <- sapply(training, as.numeric)
linReg <- xgboost(data = training,
label = NULL,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
linReg <- xgboost(objective = "reg:linear",
data = training,
label = NULL,
max.depth = 20,
eta = 0.3,
num_round = 9
)
c(1:n.train)
c(1:n.training)
c(1:nrow(training))
head(training)
linReg <- xgboost(objective = "reg:linear",
data = training,
label = AVG.WS ~ Min.WS + Max.WS + SD.WS + RA.WD + SD.WD + RA.WS,
max.depth = 20,
eta = 0.3,
num_round = 9
)
training <- data.table(trainingDat, keep.rownames = F)
linReg <- xgboost(objective = "reg:linear",
data = training,
label = AVG.WS ~ Min.WS + Max.WS + SD.WS + RA.WD + SD.WD + RA.WS,
max.depth = 20,
eta = 0.3,
num_round = 9
)
trainingDat[,-is.numeric]
trainingDat[,-is.numeric()]
is.numeric(trainingDat)
is.numeric(trainingDat[,])
is.numeric(trainingDat$AVG.WS)
is.numeric(trainingDat$Min.WS)
is.numeric(trainingDat$Max.WS)
is.numeric(trainingDat$RA.WD)
is.numeric(trainingDat$RA.WS)
is.numeric(trainingDat$SD.WS)
is.numeric(trainingDat$SD.WD)
linReg <- xgboost(objective = "reg:linear",
data = training,
label = training$AVG.WS,
max.depth = 20,
eta = 0.3,
num_round = 9
)
linReg <- xgboost(objective = "reg:linear",
data = training,
label = NULL,
max.depth = 20,
eta = 0.3,
num_round = 9
)
linReg <- xgboost(data = training,
label = NULL,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
training <- trainingDat[,]
n.training <- nrow(training)
x <- sapply(training, as.numeric)
x = matrix(as.numeric(x),nrow(x),ncol(x))
y = training[,'AVG.WS']
trind = 1:length(y)
linReg <- xgboost(data = x[trind,],
label = y,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
linReg <- xgboost(data = x[trind,],
label = y,
missing = NaN,
max.depth = 20,
eta = 0.3,
num_round = 9,
objective = "reg:linear"
)
linReg <- xgboost(data = x[trind,],
label = y,
missing = NaN,
max.depth = 20,
eta = 0.3,
nround = 9,
objective = "reg:linear"
)
linReg <- xgboost(data = x[trind,],
label = y,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
x[,is.na]
x[,is.numeric]
View(x)
linReg <- xgboost(data = x[trind,],
label = y,
missing = 0,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
x[,is.numeric]
linReg <- xgboost(data = x[trind,],
label = y,
prediction = T,
missing = 0,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
linReg <- xgboost(data = x[trind,],
label = y,
prediction = T,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
x[0,0]
x[1,1]
x[0,1]
numeric(0)
x[0,2]
linReg <- xgboost(data = training,
label = y,
prediction = T,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
linReg <- xgboost(data = training,
label = NULL,
prediction = T,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
linReg <- xgboost(data = training,
label = training$AVG.WS ~ .,
prediction = T,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
linReg <- xgboost(data = training,
label = training$AVG.WS ~ .,
prediction = T,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
training <- data.table(trainingDat, keep.rownames = F)
trainingLabels <- scale(trainingDat[,'AVG.WS'])
View(trainingLabels)
trainingLabels <- scale(trainingDat[,'AVG.WS'], center = FALSE, scale = TRUE)
View(trainingLabels)
trainingLabels <- scale(trainingDat[,'AVG.WS'], center = TRUE, scale = FALSE)
View(trainingLabels)
range01 <- function(x) {(x-min(x))/(max(x)-min(x))}
trainingLabels <- range01(trainingDat[,'AVG.WS'])
View(trainingDat)
library(scales)
trainingLabels <- rescale(trainingDat[,'AVG.WS'], to=c(0,1))
trainingDat <- trainingDat[,-'AVG.WS']
trainingDat <- trainingDat[,-AVG.WS]
trainingDat <- trainingDat[,-'AVG.WS']
trainingDat <- trainingDat[,-c('AVG.WS')]
trainingDat <- trainingDat[,select = -c('AVG.WS')]
trainingDat <- trainingDat[,select = -'AVG.WS']
trainingDat <- subset(trainingDat, select = -'AVG.WS')
trainingDat <- subset(trainingDat, select = -c('AVG.WS'))
trainingDat <- subset(trainingDat, select = -c(AVG.WS))
training <- data.table(trainingDat, keep.rownames = F)
linReg <- xgboost(data = training,
label = trainingLabels,
prediction = T,
missing = NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
linReg <- xgboost(data = training,
label = trainingLabels,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
linReg <- xgboost(data = data.matrix(training),
label = trainingLabels,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
is.na(data.matrix(training))
which is.na(data.matrix(training))
which(is.na(data.matrix(training)))
trainingMat <- data.matrix(training)
View(trainingMat)
View(trainingDat)
linReg <- xgboost(data = data.matrix(training),
label = trainingLabels,
missing=NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
testingLabels <- rescale(testingDat[,'AVG.WS'], to=c(0,1))
testingDat <- subset(testingDat, select = -c(AVG.WS))
testing <- data.table(testingDat, keep.rownames = F)
pred <- predict(linReg, newdata = data.matrix(testingDat))
confusionMatrix(data = pred, testingLabels)
linReg <- xgboost(data = data.matrix(training),
label = NULL,
missing=NaN,
max.depth = 10,
eta = 0.3,
nrounds = 250,
objective = "reg:linear"
)
confusionMatrix(data = pred, testingLabels)
pred
pred[11000:12032]
pred
trainingLabels
options(na.action = 'na.pass')
trainingDat <- read.csv(
file = "training_task2.csv",
header = TRUE,
stringsAsFactors = FALSE
)
testingDat <- read.csv(
file = "testing_task2.csv",
header = TRUE,
stringsAsFactors = FALSE
)
training <- data.table(trainingDat, keep.rownames = F)
testing <- data.table(testingDat, keep.rownames = F)
options(na.action = 'na.pass')
x <- sparse.model.matrix(~ . - 1, data = training[,-c('AVG.WS')])
x <- sparse.model.matrix(~ . - 1, data = training[,-c('AVG.WS'), with = F])
options(na.action = 'na.omit')
y <- training[,AVG.WS]
d_train <- xgb.DMatrix(data = x, label = y, missing = NA)
d_train <- xgb.DMatrix(data = x, label = y, missing = NaN)
d_train
params <- list(
objective = 'reg:linear',
eval_metric = 'rmse',
max_depth = 6,
eta = 0.3
)
linReg <- xgboost(data = d_train,
params = params,
maximize = FALSE,
watchlist = list(train = d_train),
nrounds = 25)
pred <- predict(linReg, newdata = data.matrix(testingDat))
rmse <- sqrt(sum(mean((y - pred)^2)))
pred <- predict(linReg, d_train)
rmse <- sqrt(sum(mean((y - pred)^2)))
d_train <- xgb.DMatrix(data = x, label = y, missing = NA)
params <- list(
objective = 'reg:linear',
eval_metric = 'rmse',
max_depth = 6,
eta = 0.3
)
linReg <- xgboost(data = d_train,
params = params,
maximize = FALSE,
watchlist = list(train = d_train),
nrounds = 25)
pred <- predict(linReg, d_train)
rmse <- sqrt(sum(mean((y - pred)^2)))
x <- sparse.model.matrix(~ . - AVG.WS, data = training[,-c('AVG.WS'), with = F])
x <- sparse.model.matrix(~ . - 'AVG.WS', data = training[,-c('AVG.WS'), with = F])
trainingDat <- read.csv(
file = "training_task2.csv",
header = TRUE,
stringsAsFactors = FALSE
)
testingDat <- read.csv(
file = "testing_task2.csv",
header = TRUE,
stringsAsFactors = FALSE
)
training <- data.table(trainingDat, keep.rownames = F)
testing <- data.table(testingDat, keep.rownames = F)
options(na.action = 'na.pass')
x <- sparse.model.matrix(~ . - 'AVG.WS', data = training[,-c('AVG.WS'), with = F])
x <- sparse.model.matrix(~ . - 1, data = training[,-c('AVG.WS'), with = F])
options(na.action = 'na.omit')
y <- training[,AVG.WS]
d_train <- xgb.DMatrix(data = x, label = y, missing = NA)
options(na.action = 'na.pass')
testX <- sparse.model.matrix(~ . - 1, data =testing[,-c('AVG.WS'), with = F])
options(na.action = 'na.omit')
test_y <- testing[,AVG.WS]
testY <- testing[,AVG.WS]
d_test <- xgb.DMatrix(data = testX, label = testY, missing = NA)
params <- list(
objective = 'reg:linear',
eval_metric = 'rmse',
max_depth = 6,
eta = 0.3
)
linReg <- xgboost(data = d_train,
params = params,
maximize = FALSE,
watchlist = list(eval = d_test, train = d_train),
nrounds = 25)
pred <- predict(linReg, d_test)
rmse <- sqrt(sum(mean((y - pred)^2)))
rmse <- sqrt(sum(mean((testY - pred)^2)))
points(training$AVG.WS, pred, col = "blue", pch = 4)
points(testing$AVG.WS, pred, col = "blue", pch = 4)
plot(testing, pch = 16)
points(testing$AVG.WS, pred, col = "blue", pch = 4)
linReg$residuals
rmseFn <- function(e) {
sqrt(mean(e^2))
}
error <- testY - pred
rsmeFn(error)
rmseFn(error)
mse <- sum(mean((testY - pred)^2))
pred <- predict(linReg, d_train)
rmse <- sqrt(sum(mean((y - pred)^2)))
mse <- sum(mean((y - pred)^2))
pred <- predict(linReg, d_test)
rmse <- sqrt(sum(mean((testY - pred)^2)))
mse <- sum(mean((testY - pred)^2))
View(training)
View(training)
mean(y)
params <- list(
objective = 'reg:linear',
eval_metric = 'rmse',
max_depth = 1,
eta = 0.3
)
linReg <- xgboost(data = d_train,
params = params,
maximize = FALSE,
watchlist = list(eval = d_test, train = d_train),
nrounds = 25)
pred <- predict(linReg, d_test)
rmse <- sqrt(sum(mean((testY - pred)^2)))
mse <- sum(mean((testY - pred)^2))
params <- list(
objective = 'reg:linear',
eval_metric = 'rmse',
max_depth = 25,
eta = 0.3
)
linReg <- xgboost(data = d_train,
params = params,
maximize = FALSE,
watchlist = list(eval = d_test, train = d_train),
nrounds = 25)
pred <- predict(linReg, d_test)
rmse <- sqrt(sum(mean((testY - pred)^2)))
mse <- sum(mean((testY - pred)^2))
time("1/1/2013  1:00:00 AM")
time("1/1/2013  1:00:00 AM")
time("1/1/2013  2:00:00 AM")
time_trans('1/1/2013  1:00:00 AM')
time()
time(1/6/2013 3:00)
time('1/6/2013 3:00')
time('1/6/2013 4:00')
library(forecast)
install.packages("forecast")
library(forecast)
timeDate('1/6/2013 3:00')
library(forecast)
library(caret)
